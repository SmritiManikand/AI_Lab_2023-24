# Ex.No: 10 Learning â€“ Use Supervised Learning  
### DATE: 22.04.2024                                                                           
### REGISTER NUMBER : 212221040157
### AIM: 
To write a python program to train a model to perform sentiment analysis on social media data using machine learning techniques.

###  Algorithm:

1. Collect a dataset of social media posts with labeled sentiment (positive, negative).

2. Tokenize each social media post into individual words.

3. Represent each social media post as a feature vector using the presence of words as features.

4. Split the dataset into training and testing sets.

5. Train a machine learning classifier (e.g., Naive Bayes) on the labeled training dataset.

6. Evaluate the trained classifier's accuracy on the testing dataset.

7. Visualize the classifier's performance using a confusion matrix.

8. Plot an ROC curve to assess the classifier's ability to discriminate between positive and negative sentiments.

### Program:

```
pip install nltk

import nltk
import random
from nltk.corpus import twitter_samples
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import classify
from nltk import NaiveBayesClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Download NLTK resources (if not already downloaded)
nltk.download('twitter_samples')
nltk.download('punkt')
nltk.download('stopwords')

# Load positive and negative tweets dataset from NLTK
positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')

# Tokenization, stopword removal, and stemming
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess(tweet):
    tokens = word_tokenize(tweet)
    clean_tokens = [stemmer.stem(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalnum()]
    return dict([token, True] for token in clean_tokens)

# Preprocess positive and negative tweets
positive_tweets_processed = [(preprocess(tweet), 1) for tweet in positive_tweets]  # 1 for Positive
negative_tweets_processed = [(preprocess(tweet), 0) for tweet in negative_tweets]  # 0 for Negative

# Combine positive and negative tweets, shuffle them, and split into training and testing sets
dataset = positive_tweets_processed + negative_tweets_processed
random.shuffle(dataset)
train_data = dataset[:7000]
test_data = dataset[7000:]

# Train Naive Bayes classifier
classifier = NaiveBayesClassifier.train(train_data)

# Evaluate classifier accuracy
accuracy = classify.accuracy(classifier, test_data)
print("Accuracy:", accuracy)

# Generate Confusion Matrix
actual_labels = [label for _, label in test_data]
predicted_labels = [classifier.classify(features) for features, _ in test_data]
cm = confusion_matrix(actual_labels, predicted_labels)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = range(2)
plt.xticks(tick_marks, ['Negative', 'Positive'], rotation=45)
plt.yticks(tick_marks, ['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
for i in range(2):
    for j in range(2):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > cm.max() / 2 else "black")
plt.tight_layout()

# Generate ROC Curve
probs = classifier.prob_classify_many(features for features, _ in test_data)
preds = [prob.prob(1) for prob in probs]  # 1 for Positive
fpr, tpr, _ = roc_curve(actual_labels, preds)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

### Output:


### Result:
Thus the system was trained successfully and the prediction was carried out.
